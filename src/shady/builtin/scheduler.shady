type TreeNode = struct {
    mask_t threads;
    i32 depth;
};

type JoinPoint = struct {
    TreeNode node;
    i32 destination;
};

const i32 MASK_SIZE = SUBGROUP_SIZE;

subgroup i32 next_fn;
subgroup mask_t next_mask;

subgroup i32 scheduler_cursor = 0;
subgroup [TreeNode; MASK_SIZE] scheduler_vector;

subgroup [i32; MASK_SIZE] resume_at;
// subgroup [mask_t; MASK_SIZE] resume_with;
subgroup bool fail = false;

@Builtin @DisablePass("lower_cf_instrs") @DisablePass("lower_tailcalls") @DisablePass("lower_callf")
fn builtin_create_control_point uniform JoinPoint(uniform i32 join_destination) {
    val curr_mask = subgroup_active_mask();
    val depth = scheduler_vector.(subgroup_local_id()).1;
    val tree_node = make[TreeNode]((curr_mask, depth));
    val jp = make[JoinPoint]((tree_node, join_destination));

    // increase the depth of the active leaves
    scheduler_vector.(subgroup_local_id()).1 = scheduler_vector.(subgroup_local_id()).1 + 1;

    return (jp);
}

@Builtin @DisablePass("lower_cf_instrs") @DisablePass("lower_tailcalls") @DisablePass("lower_callf")
fn builtin_fork(varying i32 branch_destination) {
    // First ask the first thread where it thinks we should branch, and all the threads that agree are set to go there
    val first_branch = subgroup_broadcast_first(branch_destination);
    if (first_branch == branch_destination) {
        next_fn = branch_destination;
        next_mask = subgroup_active_mask();

        // tag those variables as not in use.
        // resume_at.(subgroup_local_id()) = -1;
        // resume_with.(subgroup_local_id()) = empty_mask();
        return ();
    }

    // We're left with the threads we can't schedule right now.
    loop() {
        // update depth counter
        val old_depth = scheduler_vector.(subgroup_local_id()).1;
        scheduler_vector.(subgroup_local_id()).1 = old_depth + 1;

        val elected = subgroup_broadcast_first(branch_destination);
        if (elected == branch_destination) {
            resume_at.(subgroup_local_id()) = elected;
            scheduler_vector.(subgroup_local_id()).0 = subgroup_ballot(elected == branch_destination);
            break;
        }
    }
}

@Builtin @DisablePass("lower_cf_instrs") @DisablePass("lower_tailcalls") @DisablePass("lower_callf")
fn builtin_yield(uniform i32 resume_target) {
    resume_at.(subgroup_local_id()) = resume_target;
    // resume_with.(subgroup_local_id()) = subgroup_active_mask();

    // only one thread runs that part
    if (subgroup_elect_first()) {
        // bump the cursor
        // TODO bump it in a smarter way
        scheduler_cursor = (scheduler_cursor + 1) % SUBGROUP_SIZE;
        builtin_find_schedulable_leaf();
    }
}

@Builtin @DisablePass("lower_cf_instrs") @DisablePass("lower_tailcalls") @DisablePass("lower_callf")
fn builtin_join(uniform i32 join_at, uniform TreeNode token) {
    resume_at.(subgroup_local_id()) = join_at;
    scheduler_vector.(subgroup_local_id()) = token;

    // only one thread runs that part
    if (subgroup_elect_first()) {
        builtin_find_schedulable_leaf();
    }
}

@Builtin @DisablePass("lower_cf_instrs") @DisablePass("lower_tailcalls") @DisablePass("lower_callf")
fn is_parent bool(varying TreeNode child, varying TreeNode maybe_parent) {
  val child_mask = child.0;
  val parent_mask = maybe_parent.0;
  if ((child_mask | parent_mask) != parent_mask) { return(false); }
  val child_depth = child.1;
  val parent_depth = maybe_parent.1;
  return (child_depth <= parent_depth);
}

@Builtin @DisablePass("lower_cf_instrs") @DisablePass("lower_tailcalls") @DisablePass("lower_callf")
fn forward_distance i32(varying i32 x, varying i32 dst, varying i32 max_mod) {
  var i32 t = dst - x;
  if (t < 0) {
    t = t + max_mod;
  }
  return (t);
}

@Builtin @DisablePass("lower_cf_instrs") @DisablePass("lower_tailcalls") @DisablePass("lower_callf")
fn reduce2 i32(varying i32 a_index, varying i32 b_index) {
    val a = scheduler_vector.a_index;
    val b = scheduler_vector.b_index;
    
    if (is_parent(a, b)) { return (a_index); }
    if (is_parent(b, a)) { return (b_index); }
    
    val a_dist = forward_distance(a_index, scheduler_cursor, MASK_SIZE);
    val b_dist = forward_distance(b_index, scheduler_cursor, MASK_SIZE);
    
    if (a_dist < b_dist) { return (a_index); }
    return (b_index);
}

@Builtin @DisablePass("lower_cf_instrs") @DisablePass("lower_tailcalls") @DisablePass("lower_callf")
fn builtin_find_schedulable_leaf() {
    var i32 reduced = 0;
    loop (uniform i32 i = 1) {
        if (i >= MASK_SIZE) { break; }
        reduced = reduce2(reduced, i);
        continue(i + 1);
    }

    next_fn = resume_at.reduced;
    next_mask = scheduler_vector.reduced.0;
    return ();
}
